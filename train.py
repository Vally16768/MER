from __future__ import annotations

import argparse
import copy
import os
import sys
from datetime import datetime, timezone
from typing import Any
from collections import Counter

import numpy as np
import torch
from sklearn.utils.class_weight import compute_class_weight


REPO_ROOT = os.path.dirname(os.path.abspath(__file__))
SRC_DIR = os.path.join(REPO_ROOT, "src")
sys.path.insert(0, SRC_DIR)

from config_utils import build_run_name, expand_path, normalize_modalities, resolve_run_paths  # noqa: E402
from config_utils import apply_overrides, load_yaml, save_yaml  # noqa: E402
from data.dataset import AVTDictDataset, collate_avt_dict  # noqa: E402
from logging_utils import CSVLogger, ensure_dir, get_git_commit_hash, list_feature_files, load_checkpoint, save_checkpoint  # noqa: E402
from models.flexible_at import FlexibleATModel  # noqa: E402
from models.robust_at import RobustATModel  # noqa: E402
from plotting import plot_train_val_curves  # noqa: E402
from reproducibility import set_seed  # noqa: E402
from train_loop import train_val_loop  # noqa: E402
from ema import EMA  # noqa: E402


def _parse_args() -> argparse.Namespace:
    p = argparse.ArgumentParser(description="Train MER model (train + validation only).")
    p.add_argument("--config", required=True, help="Path to YAML config.")
    p.add_argument(
        "--set",
        action="append",
        default=[],
        help="Override config values as KEY=VALUE (VALUE parsed as YAML). Repeatable.",
    )
    p.add_argument("--modalities", nargs="+", choices=["A", "T"], help="Override modality set.")
    p.add_argument("--seed", type=int, help="Override seed.")
    p.add_argument("--run_name", type=str, help="Override run name (default: <dataset>_<modalities>_<timestamp>).")
    return p.parse_args()


def _select_device(device_str: str) -> torch.device:
    device_str = (device_str or "cpu").strip().lower()
    if device_str == "auto":
        device_str = "cuda" if torch.cuda.is_available() else "cpu"

    if device_str.startswith("cuda") and not torch.cuda.is_available():
        print(
            "WARNING: training.device is CUDA but torch.cuda.is_available() is False. "
            "Falling back to CPU. Install a CUDA-enabled PyTorch build to use the GPU.",
            file=sys.stderr,
        )
        return torch.device("cpu")

    return torch.device(device_str)


def _resolve_repo_path(path: str) -> str:
    path = expand_path(path)
    if not os.path.isabs(path):
        path = os.path.join(REPO_ROOT, path)
    return os.path.normpath(path)


def _quick_feature_sanity(feature_files: list[str], *, modalities: list[str], sample_n: int = 64) -> None:
    import pickle
    import random

    import numpy as np

    if not feature_files:
        return

    n = min(int(sample_n), len(feature_files))
    sample = random.sample(feature_files, k=n) if n < len(feature_files) else list(feature_files[:n])

    def _is_all_zero(x: Any) -> bool:
        try:
            arr = np.asarray(x)
            if arr.size == 0:
                return True
            return bool(np.allclose(arr, 0.0))
        except Exception:
            return False

    zero_audio = zero_text = 0
    for p in sample:
        with open(p, "rb") as f:
            d = pickle.load(f)
        if "A" in modalities:
            if _is_all_zero(d.get("audio_features")):
                zero_audio += 1
        if "T" in modalities:
            if _is_all_zero(d.get("text_features")):
                zero_text += 1

    if "A" in modalities and zero_audio == n:
        print(
            "WARNING: audio_features look all-zero in a quick sample. "
            "If these were generated by extract_at_features.py, re-run it with `--overwrite` to recompute audio features.",
            file=sys.stderr,
        )
    if "T" in modalities and zero_text == n:
        print(
            "WARNING: text_features look all-zero in a quick sample. "
            "Check transcript extraction / feature generation.",
            file=sys.stderr,
        )


def _dataset_from_feature_path(path: str) -> str:
    base = os.path.basename(path)
    stem, _ = os.path.splitext(base)
    if "_" in stem:
        return stem.split("_", 1)[0]
    return "UNKNOWN"


def _filter_feature_files(
    files: list[str],
    *,
    include_datasets: list[str] | None = None,
    exclude_datasets: list[str] | None = None,
) -> list[str]:
    include = {str(x).strip().upper() for x in (include_datasets or []) if str(x).strip()}
    exclude = {str(x).strip().upper() for x in (exclude_datasets or []) if str(x).strip()}

    if include and exclude and (include & exclude):
        overlap = sorted(include & exclude)
        raise ValueError(f"data.filter include/exclude overlap: {overlap}")

    if not include and not exclude:
        return files

    out: list[str] = []
    for p in files:
        ds = _dataset_from_feature_path(p).strip().upper()
        if include and ds not in include:
            continue
        if exclude and ds in exclude:
            continue
        out.append(p)
    return out


def _infer_feature_dims(feature_file: str, *, modalities: list[str]) -> dict[str, int]:
    import pickle

    with open(feature_file, "rb") as f:
        d = pickle.load(f)
    if not isinstance(d, dict):
        raise ValueError(f"Expected dict in feature file {feature_file}, got {type(d)}")

    def _dim(x: Any) -> int:
        arr = np.asarray(x)
        if arr.ndim == 0:
            return 1
        if arr.ndim >= 2 and arr.shape[0] == 1:
            arr = arr[0]
        return int(arr.reshape(-1).shape[0])

    out: dict[str, int] = {}
    if "A" in modalities:
        if "audio_features" not in d:
            raise ValueError(f"Missing audio_features in {feature_file}")
        out["A"] = _dim(d["audio_features"])
    if "T" in modalities:
        if "text_features" not in d:
            raise ValueError(f"Missing text_features in {feature_file}")
        out["T"] = _dim(d["text_features"])
    return out


def _load_init_checkpoint(model: torch.nn.Module, *, path: str) -> None:
    ckpt_path = _resolve_repo_path(path)
    if not os.path.isfile(ckpt_path):
        raise FileNotFoundError(f"training.init_ckpt not found: {ckpt_path}")
    ckpt_obj = load_checkpoint(ckpt_path, map_location="cpu")
    if isinstance(ckpt_obj, dict) and "model_state" in ckpt_obj:
        state = ckpt_obj["model_state"]
    elif isinstance(ckpt_obj, dict):
        state = ckpt_obj
    else:
        raise ValueError("Unsupported checkpoint format for init_ckpt")
    missing, unexpected = model.load_state_dict(state, strict=False)
    if missing or unexpected:
        print(
            f"Loaded init_ckpt with strict=False (missing={len(missing)} unexpected={len(unexpected)}).",
            file=sys.stderr,
        )


def _build_train_sampler(
    train_files: list[str],
    train_labels: list[int] | None,
    sampling_cfg: dict | None,
) -> torch.utils.data.Sampler | None:
    if not sampling_cfg:
        return None

    sampler_type = str(sampling_cfg.get("type", "none")).strip().lower()
    if sampler_type in {"none", "", "null"}:
        return None

    valid = {"dataset_balanced", "weighted", "class_balanced", "dataset_class_balanced"}
    if sampler_type not in valid:
        raise ValueError(f"Unknown training.sampling.type: {sampler_type!r}. Valid: {sorted(valid)}")

    datasets = [_dataset_from_feature_path(p) for p in train_files]
    ds_counts = Counter(datasets)

    dataset_weights: dict[str, float] = {}
    if sampler_type in {"dataset_balanced", "weighted", "dataset_class_balanced"}:
        if sampler_type == "dataset_balanced":
            dataset_weights = {d: 1.0 for d in ds_counts}
        else:
            raw = sampling_cfg.get("dataset_weights", {}) or {}
            dataset_weights = {str(k): float(v) for k, v in dict(raw).items()}

    weights = []
    class_counts: Counter[int] | None = None
    class_power = float(sampling_cfg.get("class_power", 1.0) or 1.0)
    if sampler_type in {"class_balanced", "dataset_class_balanced"}:
        if train_labels is None:
            raise ValueError("sampling.type requires train labels but train_labels is None")
        class_counts = Counter(int(x) for x in train_labels)

    for i, d in enumerate(datasets):
        w = 1.0
        if sampler_type in {"dataset_balanced", "weighted", "dataset_class_balanced"}:
            w *= float(dataset_weights.get(d, 1.0)) / float(ds_counts[d])
        if sampler_type in {"class_balanced", "dataset_class_balanced"}:
            assert train_labels is not None and class_counts is not None
            y = int(train_labels[i])
            w *= 1.0 / float(class_counts[y] ** class_power)
        weights.append(float(w))

    num_samples = int(sampling_cfg.get("num_samples", len(train_files)))
    replacement = bool(sampling_cfg.get("replacement", True))

    return torch.utils.data.WeightedRandomSampler(weights=weights, num_samples=num_samples, replacement=replacement)


def main() -> None:
    args = _parse_args()

    cfg = load_yaml(args.config)
    cfg = apply_overrides(cfg, args.set)

    if args.modalities is not None:
        cfg["modalities"] = args.modalities

    cfg.setdefault("training", {})
    if args.seed is not None:
        cfg["training"]["seed"] = int(args.seed)

    dataset_cfg = cfg.get("dataset", {})
    dataset_name = str(dataset_cfg.get("name", "dataset"))
    num_classes = int(dataset_cfg.get("num_classes", 0))
    if num_classes <= 1:
        raise ValueError("Config must set dataset.num_classes (>=2).")

    modalities = normalize_modalities(cfg.get("modalities", []))

    data_cfg = cfg.get("data", {})
    train_dir = _resolve_repo_path(str(data_cfg.get("train_dir", "")))
    val_dir = _resolve_repo_path(str(data_cfg.get("val_dir", "")))
    if not train_dir or not os.path.isdir(train_dir):
        raise FileNotFoundError(f"Invalid data.train_dir: {train_dir!r}")
    if not val_dir or not os.path.isdir(val_dir):
        raise FileNotFoundError(f"Invalid data.val_dir: {val_dir!r}")

    backend = str(data_cfg.get("backend", "pickle")).strip().lower()
    if backend not in {"pickle", "memmap"}:
        raise ValueError("data.backend must be 'pickle' or 'memmap'")

    training_cfg = cfg.get("training", {})
    seed = int(training_cfg.get("seed", 42))
    set_seed(seed)

    device = _select_device(str(training_cfg.get("device", "cpu")))

    output_cfg = cfg.get("output", {})
    output_root = _resolve_repo_path(str(output_cfg.get("root_dir", "outputs")))
    run_name = args.run_name or output_cfg.get("run_name") or build_run_name(dataset_name, modalities)
    run_paths = resolve_run_paths(output_root, run_name)

    ensure_dir(run_paths.run_dir)
    ensure_dir(run_paths.checkpoints_dir)
    ensure_dir(run_paths.plots_dir)

    resolved_cfg = copy.deepcopy(cfg)
    resolved_cfg["modalities"] = modalities
    resolved_cfg.setdefault("output", {})
    resolved_cfg["output"]["root_dir"] = output_root
    resolved_cfg["output"]["run_name"] = run_name
    resolved_cfg["output"]["run_dir"] = run_paths.run_dir
    resolved_cfg.setdefault("meta", {})
    resolved_cfg["meta"]["resolved_at_utc"] = datetime.now(tz=timezone.utc).isoformat()
    resolved_cfg["meta"]["git_commit"] = get_git_commit_hash(REPO_ROOT)
    resolved_cfg["meta"]["config_path"] = os.path.abspath(args.config)
    resolved_cfg["meta"]["cli_overrides"] = list(args.set)
    save_yaml(resolved_cfg, run_paths.resolved_config_yaml)

    feature_exts = data_cfg.get("extensions", [".pkl", ".pickle"])
    filter_cfg = data_cfg.get("filter", {}) or {}

    if backend == "pickle":
        train_files = list_feature_files(train_dir, extensions=feature_exts)
        val_files = list_feature_files(val_dir, extensions=feature_exts)
        if not train_files:
            raise FileNotFoundError(f"No feature files found in train_dir={train_dir!r}")
        if not val_files:
            raise FileNotFoundError(f"No feature files found in val_dir={val_dir!r}")

        train_files = _filter_feature_files(
            train_files,
            include_datasets=filter_cfg.get("include_datasets_train", filter_cfg.get("include_datasets")),
            exclude_datasets=filter_cfg.get("exclude_datasets_train", filter_cfg.get("exclude_datasets")),
        )
        val_files = _filter_feature_files(
            val_files,
            include_datasets=filter_cfg.get("include_datasets_val", filter_cfg.get("include_datasets")),
            exclude_datasets=filter_cfg.get("exclude_datasets_val", filter_cfg.get("exclude_datasets")),
        )
        if not train_files:
            raise FileNotFoundError("After filtering, no train files remain. Check data.filter.include_datasets_*.")
        if not val_files:
            raise FileNotFoundError("After filtering, no val files remain. Check data.filter.include_datasets_*.")

        _quick_feature_sanity(train_files, modalities=modalities)

        train_ds = AVTDictDataset(train_files, modalities=modalities)
        val_ds = AVTDictDataset(val_files, modalities=modalities)

        inferred = _infer_feature_dims(train_files[0], modalities=modalities)
        train_labels_list = None
        train_ids_for_sampling = train_files
    else:
        from data.memmap_at import MemmapATDataset

        if filter_cfg:
            raise ValueError("data.filter is not supported for data.backend=memmap (build separate memmaps per subset).")

        train_ds = MemmapATDataset(root_dir=train_dir, modalities=modalities)
        val_ds = MemmapATDataset(root_dir=val_dir, modalities=modalities)

        inferred = {}
        if "A" in modalities:
            inferred["A"] = int(train_ds.meta.audio_dim or 0)
        if "T" in modalities:
            inferred["T"] = int(train_ds.meta.text_dim or 0)

        # For sampler/class-weights, read labels and ids once.
        train_labels_list = [int(x) for x in np.asarray(train_ds.labels).reshape(-1).tolist()]
        if train_ds.meta.ids_path:
            ids_path = os.path.join(train_dir, train_ds.meta.ids_path)
            with open(ids_path, "r", encoding="utf-8") as f:
                ids = [line.strip() for line in f if line.strip()]
        else:
            ids = [str(i) for i in range(len(train_ds))]
        train_ids_for_sampling = [f"{x}.pkl" for x in ids]

    batch_size = int(training_cfg.get("batch_size", 32))
    num_workers = int(data_cfg.get("num_workers", 0))
    pin_memory = device.type == "cuda"

    # Load labels once if they are needed for either class weights or sampling.
    class_weights_mode = training_cfg.get("class_weights", "balanced")
    sampling_cfg = training_cfg.get("sampling", None)
    sampler_type = str(sampling_cfg.get("type", "none")).strip().lower() if isinstance(sampling_cfg, dict) else "none"
    needs_train_labels = bool(class_weights_mode in ("balanced", True) or sampler_type in {"class_balanced", "dataset_class_balanced"})
    if needs_train_labels and train_labels_list is None:
        train_labels_list = [int(train_ds[i]["label"]) for i in range(len(train_ds))]

    train_sampler = None
    if isinstance(sampling_cfg, dict):
        train_sampler = _build_train_sampler(train_ids_for_sampling, train_labels_list, sampling_cfg)

    loader_cfg = data_cfg.get("loader", {}) or {}
    persistent_workers = bool(loader_cfg.get("persistent_workers", num_workers > 0))
    prefetch_factor = int(loader_cfg.get("prefetch_factor", 2))

    loader_kwargs: dict[str, Any] = {}
    if num_workers > 0:
        loader_kwargs["persistent_workers"] = bool(persistent_workers)
        loader_kwargs["prefetch_factor"] = int(prefetch_factor)

    train_loader = torch.utils.data.DataLoader(
        train_ds,
        batch_size=batch_size,
        shuffle=train_sampler is None,
        sampler=train_sampler,
        num_workers=num_workers,
        pin_memory=pin_memory,
        collate_fn=collate_avt_dict,
        **loader_kwargs,
    )
    val_loader = torch.utils.data.DataLoader(
        val_ds,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=pin_memory,
        collate_fn=collate_avt_dict,
        **loader_kwargs,
    )

    model_cfg = cfg.get("model", {})
    if "A" in modalities:
        expected = int(model_cfg.get("input_dim_a", 0) or 0)
        if expected <= 0:
            raise ValueError("Config must set model.input_dim_a when training with audio modality.")
        if expected != int(inferred["A"]):
            raise ValueError(
                f"Feature dim mismatch: model.input_dim_a={expected} but audio_features dim is {inferred['A']} "
                f"(example: {os.path.basename(train_files[0])}). "
                "Re-extract features with matching n_mfcc or set --set model.input_dim_a=<dim>."
            )
    if "T" in modalities:
        expected = int(model_cfg.get("input_dim_t", 0) or 0)
        if expected <= 0:
            raise ValueError("Config must set model.input_dim_t when training with text modality.")
        if expected != int(inferred["T"]):
            raise ValueError(
                f"Feature dim mismatch: model.input_dim_t={expected} but text_features dim is {inferred['T']} "
                f"(example: {os.path.basename(train_files[0])})."
            )

    model_type = str(model_cfg.get("type", "flexible_at")).strip().lower()
    if model_type in {"flexible_at", "gfl", "baseline"}:
        model = FlexibleATModel(
            input_dim_audio=int(model_cfg.get("input_dim_a", 512)),
            input_dim_text=int(model_cfg.get("input_dim_t", 768)),
            gated_dim=int(model_cfg.get("gated_dim", 128)),
            n_classes=num_classes,
            drop=float(model_cfg.get("dropout", 0.0)),
            modalities=modalities,
        ).to(device)
    elif model_type in {"robust_at", "robust"}:
        hidden_dim = int(model_cfg.get("hidden_dim", model_cfg.get("gated_dim", 256)))
        model = RobustATModel(
            input_dim_audio=int(model_cfg.get("input_dim_a", 512)),
            input_dim_text=int(model_cfg.get("input_dim_t", 768)),
            hidden_dim=hidden_dim,
            n_classes=num_classes,
            num_layers=int(model_cfg.get("num_layers", 4)),
            num_heads=int(model_cfg.get("num_heads", 8)),
            ffn_mult=int(model_cfg.get("ffn_mult", 4)),
            modalities=modalities,
        ).to(device)
    else:
        raise ValueError(f"Unknown model.type: {model_type!r}")

    init_ckpt = training_cfg.get("init_ckpt", None)
    if init_ckpt:
        _load_init_checkpoint(model, path=str(init_ckpt))

    label_smoothing = float(training_cfg.get("label_smoothing", 0.0) or 0.0)
    if class_weights_mode in ("balanced", True):
        if train_labels_list is None:
            train_labels_list = [int(train_ds[i]["label"]) for i in range(len(train_ds))]
        train_labels = np.asarray(train_labels_list, dtype=int)
        present = np.unique(train_labels)
        weights_present = compute_class_weight("balanced", classes=present, y=train_labels)
        weights = np.ones((num_classes,), dtype=np.float32)
        weights[present] = weights_present.astype(np.float32, copy=False)
        weight_t = torch.tensor(weights, dtype=torch.float32, device=device)
        criterion = torch.nn.CrossEntropyLoss(weight=weight_t, label_smoothing=label_smoothing)
    elif class_weights_mode in (None, False, "none"):
        criterion = torch.nn.CrossEntropyLoss(label_smoothing=label_smoothing)
    else:
        raise ValueError("training.class_weights must be 'balanced' or null/none/false")

    opt_name = str(training_cfg.get("optimizer", "adam")).lower()
    lr = float(training_cfg.get("learning_rate", 1e-4))
    wd = float(training_cfg.get("weight_decay", 0.0))
    if opt_name == "adam":
        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)
    elif opt_name == "adamw":
        optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)
    elif opt_name == "sgd":
        momentum = float(training_cfg.get("momentum", 0.9))
        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=wd)
    elif opt_name == "lion":
        from lion_pytorch import Lion

        optimizer = Lion(model.parameters(), lr=lr, weight_decay=wd)
    else:
        raise ValueError(f"Unknown optimizer: {opt_name!r}")

    scheduler = None
    scheduler_step_per_batch = False
    scheduler_metric = None
    sched_cfg = training_cfg.get("scheduler", None)
    if isinstance(sched_cfg, dict):
        sched_type = str(sched_cfg.get("type", "none")).lower()
        if sched_type and sched_type != "none":
            params = {k: v for k, v in sched_cfg.items() if k != "type"}
            if sched_type == "steplr":
                scheduler = torch.optim.lr_scheduler.StepLR(optimizer, **params)
            elif sched_type == "cosineannealingwarmrestarts":
                scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, **params)
                scheduler_step_per_batch = True
            elif sched_type == "reduceonplateau":
                scheduler_metric = params.pop("metric", None)
                scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, **params)
            else:
                raise ValueError(f"Unknown scheduler type: {sched_type!r}")

    best_metric = str(training_cfg.get("best_metric", "UAR")).lower()
    best_metric = {"wf1": "wf1", "w_f1": "wf1", "weighted_f1": "wf1", "uar": "uar", "acc": "accuracy"}.get(
        best_metric, best_metric
    )

    epochs = int(training_cfg.get("epochs", 1))
    patience = training_cfg.get("patience", None)
    patience_int = int(patience) if patience is not None else None
    min_delta = float(training_cfg.get("min_delta", 0.0) or 0.0)
    grad_clip_norm = training_cfg.get("grad_clip_norm", None)
    grad_clip_norm_f = float(grad_clip_norm) if grad_clip_norm is not None else None

    train_logger = CSVLogger(
        run_paths.metrics_train_csv,
        [
            "epoch",
            "loss",
            "accuracy",
            "wf1",
            "uar",
            "lr",
            "time_epoch_sec",
            "drop_audio_rate",
            "drop_text_rate",
            "optimizer_steps",
        ],
    )
    val_logger = CSVLogger(
        run_paths.metrics_val_csv,
        ["epoch", "loss", "accuracy", "wf1", "uar", "lr", "time_epoch_sec"],
    )

    aug_cfg = training_cfg.get("augmentation", {}) or {}
    feature_noise_std_audio = float(aug_cfg.get("feature_noise_std_audio", 0.0) or 0.0)
    feature_noise_std_text = float(aug_cfg.get("feature_noise_std_text", 0.0) or 0.0)
    modality_dropout_p = float(aug_cfg.get("modality_dropout_p", 0.0) or 0.0)

    ema_cfg = training_cfg.get("ema", None)
    ema = None
    ema_use_for_eval = False
    if isinstance(ema_cfg, dict) and bool(ema_cfg.get("enabled", False)):
        ema = EMA(model, decay=float(ema_cfg.get("decay", 0.999) or 0.999))
        ema_use_for_eval = bool(ema_cfg.get("use_for_eval", True))

    def on_best(epoch: int, metric_value: float) -> None:
        if ema is not None and ema_use_for_eval:
            with ema.apply_to(model):
                best_state = {k: v.detach().clone() for k, v in model.state_dict().items()}
        else:
            best_state = {k: v.detach().clone() for k, v in model.state_dict().items()}
        save_checkpoint(
            os.path.join(run_paths.checkpoints_dir, "best.pt"),
            model_state=best_state,
            optimizer_state=optimizer.state_dict(),
            epoch=epoch,
            extra={"best_metric": best_metric, "best_value": metric_value},
        )

    def on_epoch_end(epoch: int, train_row: dict, val_row: dict) -> None:
        lr_now = float(optimizer.param_groups[0].get("lr", lr))
        train_row["lr"] = lr_now
        val_row["lr"] = lr_now
        train_logger.log(train_row)
        val_logger.log(val_row)
        save_checkpoint(
            os.path.join(run_paths.checkpoints_dir, "last.pt"),
            model_state=model.state_dict(),
            optimizer_state=optimizer.state_dict(),
            epoch=epoch,
            extra={"best_metric": best_metric},
        )
        t_sec = float(train_row.get("time_epoch_sec", float("nan")))
        print(
            f"[epoch {epoch:03d}] "
            f"lr={lr_now:.3e} "
            f"t={t_sec:.1f}s "
            f"train loss={train_row['loss']:.4f} acc={train_row['accuracy']:.4f} wf1={train_row['wf1']:.4f} uar={train_row['uar']:.4f} | "
            f"val loss={val_row['loss']:.4f} acc={val_row['accuracy']:.4f} wf1={val_row['wf1']:.4f} uar={val_row['uar']:.4f}"
        )

    amp = bool(training_cfg.get("amp", False))
    grad_accum_steps = int(training_cfg.get("grad_accum_steps", 1) or 1)

    result = train_val_loop(
        model=model,
        train_loader=train_loader,
        val_loader=val_loader,
        optimizer=optimizer,
        criterion=criterion,
        device=device,
        epochs=epochs,
        best_metric_name=best_metric,
        patience=patience_int,
        min_delta=min_delta,
        scheduler=scheduler,
        scheduler_metric=str(scheduler_metric) if scheduler_metric is not None else None,
        scheduler_step_per_batch=scheduler_step_per_batch,
        amp=amp,
        grad_accum_steps=grad_accum_steps,
        grad_clip_norm=grad_clip_norm_f,
        ema=ema,
        ema_use_for_eval=ema_use_for_eval,
        feature_noise_std_audio=feature_noise_std_audio,
        feature_noise_std_text=feature_noise_std_text,
        modality_dropout_p=modality_dropout_p,
        on_best=on_best,
        on_epoch_end=on_epoch_end,
    )

    plot_train_val_curves(
        metrics_train_csv=run_paths.metrics_train_csv,
        metrics_val_csv=run_paths.metrics_val_csv,
        plots_dir=run_paths.plots_dir,
    )

    print(f"Run dir: {run_paths.run_dir}")
    print(f"Best: epoch={result.best_epoch} {best_metric}={result.best_metric:.6f}")


if __name__ == "__main__":
    main()
