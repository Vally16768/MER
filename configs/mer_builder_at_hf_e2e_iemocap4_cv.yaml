dataset:
  name: IEMOCAP4_HF_E2E_CV
  num_classes: 4
  class_names:
    - anger
    - joy
    - sadness
    - neutral
  # Keep only samples whose manifest emotion is in class_names.
  drop_unknown_labels: true

modalities: [A, T]

data:
  processed_dir: mer_dataset_builder/data/processed
  # For CV we treat the whole IEMOCAP set as a pool and slice by session/speaker regex.
  train_splits: [train, val, test]
  val_splits: [train, val, test]
  eval_splits: [train, val, test]
  max_audio_sec: 12.0
  text_max_tokens: 256
  include_speaker_in_text: false
  meld_context_window: 0
  meld_context_sep: " </s> "
  num_workers: 4
  loader:
    persistent_workers: true
    prefetch_factor: 2
  filter:
    include_datasets_train: [IEMOCAP]
    include_datasets_val: [IEMOCAP]
    include_datasets_eval: [IEMOCAP]
    # These are intentionally left unset in the config:
    # - include_speaker_regex_{train,val,eval}
    # - exclude_speaker_regex_{train,val,eval}
    # The CV runner script overrides them per fold.

model:
  audio_model: microsoft/wavlm-base
  text_model: roberta-base
  pool_audio: mean_std
  pool_text: cls
  freeze_audio: false
  freeze_text: false
  freeze_audio_feature_encoder: true
  hidden_dim: 256
  num_layers: 4
  num_heads: 8
  ffn_mult: 4
  modality_dropout_p: 0.0

training:
  device: auto
  seed: 42
  batch_size: 8
  epochs: 12
  learning_rate: 1.0e-4
  lr_audio: 2.0e-5
  lr_text: 2.0e-5
  weight_decay: 1.0e-4
  optimizer: adamw
  class_weights: balanced
  label_smoothing: 0.1
  grad_clip_norm: 1.0
  amp: true
  grad_accum_steps: 4
  scheduler:
    type: reduceonplateau
    metric: uar
    mode: max
    factor: 0.5
    patience: 2
    min_lr: 1.0e-6
  patience: 4
  min_delta: 1.0e-4
  best_metric: weighted_f1

output:
  root_dir: outputs

