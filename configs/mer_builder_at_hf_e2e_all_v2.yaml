dataset:
  name: MER_BUILDER_AT_HF_E2E_ALL_V2
  num_classes: 7
  class_names:
    - anger
    - disgust
    - fear
    - joy
    - neutral
    - sadness
    - surprise

modalities: [A, T]

data:
  processed_dir: mer_dataset_builder/data/processed
  train_splits: [train, meld_train]
  val_splits: [val, meld_dev]
  eval_splits: [testB]
  # Longer clips help conversational emotion.
  max_audio_sec: 12.0
  text_max_tokens: 320
  # MELD robustness: include speakers + short dialogue history (previous utterances only).
  include_speaker_in_text: true
  meld_context_window: 5
  meld_context_sep: " </s> "
  num_workers: 4
  loader:
    persistent_workers: true
    prefetch_factor: 2
  filter:
    # Default eval focus is MELD (TestB). For TestA, override with: --set "data.filter={}"
    include_datasets_eval: [MELD]

model:
  audio_model: microsoft/wavlm-base
  text_model: roberta-base
  pool_audio: mean_std
  pool_text: cls
  freeze_audio: false
  freeze_text: false
  # Stabilize early training for wav2vec2-style encoders.
  freeze_audio_feature_encoder: true
  hidden_dim: 256
  num_layers: 4
  num_heads: 8
  ffn_mult: 4
  # Disable modality dropout for end-to-end (use after convergence if needed).
  modality_dropout_p: 0.0

training:
  device: auto
  seed: 42
  batch_size: 8
  epochs: 30
  # Lower LRs (val curve showed overfitting/instability after the peak).
  learning_rate: 1.0e-4
  lr_audio: 2.0e-5
  lr_text: 2.0e-5
  weight_decay: 1.0e-4
  optimizer: adamw
  class_weights: balanced
  label_smoothing: 0.1
  grad_clip_norm: 1.0
  amp: true
  grad_accum_steps: 4
  scheduler:
    type: reduceonplateau
    metric: uar
    mode: max
    factor: 0.5
    patience: 2
    min_lr: 1.0e-6
  # Stop closer to the best epoch (reduces wasted overfit epochs).
  patience: 4
  min_delta: 1.0e-4
  sampling:
    type: weighted
    dataset_weights:
      MELD: 6.0
  best_metric: UAR

output:
  root_dir: outputs

